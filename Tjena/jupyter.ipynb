{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKLIST\n",
    "* Fixa dataloaders\n",
    "    * Vafan är datan egentligen?\n",
    "    * \n",
    "* Fixa model\n",
    "    * Jag antar vi kör CNN (CSN plz)\n",
    "    * Prova andra arkitekturer?\n",
    "    *\n",
    "* Fixa hyperparametrar, kanske använda maskininlärning för det?\n",
    "* Fixa CUDA support och träna hemma?\n",
    "* Fixa det sista med matrisen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnc\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import netCDF4\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/skogsstyrelsen/skogs_names_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m SPLIT_TO_USE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read data + corresponding json info (incl ground truth)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m img_paths_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      8\u001b[0m img_paths_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_val.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      9\u001b[0m img_paths_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/skogsstyrelsen/skogs_names_train.npy'"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/skogsstyrelsen-data\"\n",
    "BASE_PATH_LOG = '../log'\n",
    "BASE_PATH_DATA = '../data/skogsstyrelsen'\n",
    "SPLIT_TO_USE = 'trainval' \n",
    "\n",
    "# Read data + corresponding json info (incl ground truth)\n",
    "img_paths_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_train.npy')))\n",
    "img_paths_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_val.npy')))\n",
    "img_paths_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy')))\n",
    "json_content_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_train.npy'), allow_pickle=True))\n",
    "json_content_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_val.npy'), allow_pickle=True))\n",
    "json_content_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_test.npy'), allow_pickle=True))\n",
    "# Run model on desired split\n",
    "\n",
    "if SPLIT_TO_USE == 'train':\n",
    "\timg_paths = img_paths_train\n",
    "\tjson_paths = json_content_train\n",
    "elif SPLIT_TO_USE == 'val':\n",
    "\timg_paths = img_paths_val\n",
    "\tjson_paths = json_content_val\n",
    "elif SPLIT_TO_USE == 'trainval':\n",
    "\timg_paths = img_paths_train + img_paths_val\n",
    "\tjson_paths = json_content_train + json_content_val\n",
    "elif SPLIT_TO_USE == 'test':\n",
    "\timg_paths = img_paths_test\n",
    "\tjson_paths = json_content_test\n",
    "all_binary_preds = []\n",
    "all_binary_gts = []\n",
    "for img_idx, img_path in enumerate(img_paths):\n",
    "\n",
    "\tprint(img_idx, len(img_paths))\n",
    "\n",
    "\t# Extract date to see if data is from before or after Jan 2022\n",
    "\t# (this affects the normalization used for the image)\n",
    "\timg = xr.open_dataset(img_path)\n",
    "\tyy_mm_dd = getattr(img, 'time').values[0]\n",
    "\tyy = yy_mm_dd.astype('datetime64[Y]').astype(int) + 1970\n",
    "\tmm = yy_mm_dd.astype('datetime64[M]').astype(int) % 12 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "#Creates a dataset for our cloud images\n",
    "class CloudClassificationDataset(Dataset):\n",
    "    def init(self, imagedir, json, transform=None, channels:tuple=(\"b04\",\"b03\",\"b02\")):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.channels = channels\n",
    "        self.json = np.load(json,allow_pickle=True)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.json)\n",
    "\n",
    "    def getitem(self, index):\n",
    "        # Finds filename in json file and creates path to corresponding image\n",
    "        filename = \"skgs\"+self.json[index]['ValideringsobjektBildId']+\".nc\"\n",
    "        img_path = os.path.join(self.image_dir,filename)\n",
    "\n",
    "        # Opens NetCDF4 image\n",
    "        df = xr.open_dataset(img_path, engine='netcdf4')\n",
    "\n",
    "        # Extracts labels from the json file\n",
    "        label = self.json[index][\"MolnDis\"]\n",
    "\n",
    "        #Creates a tuple of dataArray objects with given channels from the tuple of channels and the dataArray\n",
    "        channel_lst=[]\n",
    "        for c in self.channels:\n",
    "            channel_lst.append(df[c][0])\n",
    "        channel_tup = tuple(channel_lst)\n",
    "\n",
    "        # Stacks, normalizes, and cut the image to a (20, 20, C) format between 0 and 1\n",
    "        imagergb = np.dstack(channel_tup)\n",
    "        cli=0\n",
    "        image = np.clip((imagergb-np.percentile(imagergb,cli))/(np.percentile(imagergb,100-cli)-np.percentile(imagergb,cli)),0, 1)[:20,:20,:]\n",
    "\n",
    "        return image,int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CloudClassificationDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'CloudClassificationDataset' has no attribute 'CloudClassificationDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##Get the dataset of the CloudDataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCloudClassificationDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCloudClassificationDataset\u001b[49m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maffel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mD7046E - Course\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mml-cloud-opt-thick-master\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTjena\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mskogsstyrelsen\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mskogs_json_train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m##Creates validation/training dataset with 20/80 split\u001b[39;00m\n\u001b[0;32m      5\u001b[0m validation_dataset, training_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(dataset,[\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'CloudClassificationDataset' has no attribute 'CloudClassificationDataset'"
     ]
    }
   ],
   "source": [
    "##Get the dataset of the CloudDataset\n",
    "dataset = CloudClassificationDataset.CloudClassificationDataset(r\"C:\\Users\\affel\\D7046E - Course\\ml-cloud-opt-thick-master\\Tjena\\data\\skogsstyrelsen\\skogs_json_train.npy\")\n",
    "\n",
    "##Creates validation/training dataset with 20/80 split\n",
    "validation_dataset, training_dataset = torch.utils.data.random_split(dataset,[0.2, 0.8])\n",
    "\n",
    "##Create a DataLoaders from the datasets.\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "##Creates an iterator and plots the pictures\n",
    "it = iter(training_loader)\n",
    "images, labels = next(it)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "\n",
    "print(labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\affel\\\\D7046E - Course\\\\ml-cloud-opt-thick-master\\\\Tjena\\\\data\\\\skogsstyrelsen\\\\skgs174cf5d2-46c7-ed11-9174-005056a6f472.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\file_manager.py:209\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 209\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 55\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('C:\\\\Users\\\\affel\\\\D7046E - Course\\\\ml-cloud-opt-thick-master\\\\Tjena\\\\data\\\\skogsstyrelsen\\\\skgs174cf5d2-46c7-ed11-9174-005056a6f472.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'c9a97feb-a127-4f74-8540-59dc3859a7ad']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Creates an iterator and plots the pictures\u001b[39;00m\n\u001b[0;32m     62\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(training_loader)\n\u001b[1;32m---> 63\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Display the first image in the batch\u001b[39;00m\n\u001b[0;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m, in \u001b[0;36mCloudClassificationDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     20\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, filename)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Opens NetCDF4 image\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnetcdf4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Extracts labels from the json file\u001b[39;00m\n\u001b[0;32m     26\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson[index][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMolnDis\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\api.py:541\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    530\u001b[0m     decode_cf,\n\u001b[0;32m    531\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    537\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    538\u001b[0m )\n\u001b[0;32m    540\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 541\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[0;32m    548\u001b[0m     backend_ds,\n\u001b[0;32m    549\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    558\u001b[0m )\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:578\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[1;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    559\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    575\u001b[0m ):\n\u001b[0;32m    577\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m--> 578\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    376\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    377\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[0;32m    380\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    381\u001b[0m )\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:329\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[1;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m    386\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\file_manager.py:197\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\affel\\anaconda3\\envs\\nnlm\\lib\\site-packages\\xarray\\backends\\file_manager.py:215\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    213\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    214\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[1;32m--> 215\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2469\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2028\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\affel\\\\D7046E - Course\\\\ml-cloud-opt-thick-master\\\\Tjena\\\\data\\\\skogsstyrelsen\\\\skgs174cf5d2-46c7-ed11-9174-005056a6f472.nc'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "#Creates a dataset for our cloud images\n",
    "class CloudClassificationDataset(Dataset):\n",
    "    def __init__(self, imagedir, json, transform=None, channels=(\"b04\", \"b03\", \"b02\")):\n",
    "        self.image_dir = imagedir\n",
    "        self.transform = transform\n",
    "        self.channels = channels\n",
    "        self.json = np.load(json, allow_pickle=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Finds filename in json file and creates path to corresponding image\n",
    "        filename = \"skgs\" + self.json[index]['ValideringsobjektBildId'] + \".nc\"\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "\n",
    "        # Opens NetCDF4 image\n",
    "        df = xr.open_dataset(img_path, engine='netcdf4')\n",
    "\n",
    "        # Extracts labels from the json file\n",
    "        label = self.json[index][\"MolnDis\"]\n",
    "\n",
    "        # Creates a tuple of dataArray objects with given channels from the tuple of channels and the dataArray\n",
    "        channel_lst = []\n",
    "        for c in self.channels:\n",
    "            channel_lst.append(df[c][0])\n",
    "        channel_tup = tuple(channel_lst)\n",
    "\n",
    "        # Stacks, normalizes, and cut the image to a (20, 20, C) format between 0 and 1\n",
    "        imagergb = np.dstack(channel_tup)\n",
    "        cli = 0\n",
    "        image = np.clip((imagergb - np.percentile(imagergb, cli)) / (\n",
    "                    np.percentile(imagergb, 100 - cli) - np.percentile(imagergb, cli)), 0, 1)[:20, :20, :]\n",
    "\n",
    "        return image, int(label)\n",
    "\n",
    "# Path to your dataset\n",
    "image_dir = r\"C:\\Users\\affel\\D7046E - Course\\ml-cloud-opt-thick-master\\Tjena\\data\\skogsstyrelsen\"\n",
    "json_path = r\"C:\\Users\\affel\\D7046E - Course\\ml-cloud-opt-thick-master\\Tjena\\data\\skogsstyrelsen\\skogs_json_train.npy\"\n",
    "\n",
    "# Create an instance of your dataset\n",
    "dataset = CloudClassificationDataset(image_dir, json_path)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 3\n",
    "\n",
    "# Split the dataset into validation and training sets\n",
    "validation_size = int(0.2 * len(dataset))\n",
    "training_size = len(dataset) - validation_size\n",
    "validation_dataset, training_dataset = torch.utils.data.random_split(dataset, [validation_size, training_size])\n",
    "\n",
    "# Create DataLoader instances for validation and training\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Creates an iterator and plots the pictures\n",
    "it = iter(training_loader)\n",
    "images, labels = next(it)\n",
    "\n",
    "# Display the first image in the batch\n",
    "plt.figure()\n",
    "plt.imshow(images[0].numpy().transpose((1, 2, 0)))  # Transpose image to (H, W, C) for display\n",
    "plt.show()\n",
    "\n",
    "print(labels[0].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_db7d323a-3fc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c4ade691-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4265b290-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_dbd95f27-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_fcdb2f62-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cde9b181-37c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_210bf8a8-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_93702317-47c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7055b1e2-39c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_f61efa16-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_3cf023ff-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c78125b1-3cc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_0008981d-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_ce91d9e3-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_02baf951-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a555d858-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_652f74e7-3fc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_590a4af0-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_41e82d70-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_dc5ce0ee-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_fe45ae6c-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7c5279f2-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1a55ccf8-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_f6d11f3f-39c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_195c4eda-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_065101fb-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6817e9d5-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_e90a80d7-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_33d97f97-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d4b7d18f-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_0d455c60-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b5fdcb76-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7c479624-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2c50333e-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_e59ba8bc-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bdabbb27-3bc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_0b5101fb-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_0146ae6c-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a8393ae2-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_3581d482-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5f9d4a54-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_accaeac2-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_85dfa0bb-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5ae44916-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_982857ec-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d59cd189-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2342e895-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_794bbce8-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c377d0dd-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_ff210101-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1d9b58f6-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_887440af-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_39824d95-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_e261f668-37c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_677e366f-37c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_aa67f212-48c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_f186db71-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_42b98019-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_fa07981d-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b198fe78-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2842e895-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_eacff6c8-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_142727a6-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_9b0b0d1f-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_94515508-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b824b793-47c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_47dea9f2-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_557ff63e-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_68c938f7-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_98702317-47c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7dc340c5-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_47b88b11-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bb77d0dd-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_042fff0e-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_8a4f5454-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d57ea55e-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a50b0d1f-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_f7ebf8ce-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_105101fb-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c4c8e760-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_04525318-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4432757c-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_57082d52-39c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_feebf8ce-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5532230d-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_12fffcee-39c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_8b414409-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_252d8917-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_8b9c8185-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6c17e9d5-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c9d4b28a-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_9d6ab544-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a22857ec-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_93414409-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d0d26371-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cf360cea-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_49dea9f2-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a0393ae2-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_849d021c-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_3c024002-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7463d5b6-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_23fc631e-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_00788643-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_8bc79e8b-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cf460f6a-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_59ec6fa3-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_17d6954b-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_14506b93-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1156665f-48c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_63ec6fa3-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7bb1dc4b-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d97ea55e-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_84c340c5-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2931f3f4-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_dd360cea-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4d32230d-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2e35ad65-48c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_728c5649-47c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_313704f2-3bc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_e6e41e07-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_942efd11-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_980236e6-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c9d2c11f-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5132230d-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7285b76f-49c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7e6ec10b-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cb0935ce-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7bd38d7b-37c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_35aba938-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bebea249-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2c3b891d-3cc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_dcf4afa9-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d19cd189-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bce916dc-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b13cb687-37c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c7e74836-47c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_fdf52707-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_252a4730-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4302939d-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d9a18327-3fc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6f84583d-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1688bee7-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bc0a255c-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b2c558e8-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_f7840011-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_47ca7c76-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_99589cd2-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_045101fb-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5d8d586a-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_174cf5d2-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2d31f3f4-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1a58a2b5-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_970d1b98-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a3393ae2-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a9f03644-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_44ca7c76-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_aa393ae2-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_37ca7e18-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6af3c806-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6658bc99-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_86cc1e26-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_00850011-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bc82ed0e-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_e519b346-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_194cf5d2-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7061b1e6-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_937440af-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7561b1e6-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4942ff8e-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b89d8ebe-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_aa758dcf-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6ec0d515-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_dd8e8c53-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cdabbb27-3bc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b8caeac2-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b24d2df1-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1f16b04c-36c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2bf3f650-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_01a04a58-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_707e366f-37c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5578b34f-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_025101fb-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b94c1322-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c40a255c-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2afc631e-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_fd060f24-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a35ccfc5-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_3cf0d5fe-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_0955ccf8-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1e20462f-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_910d1b98-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_dd19b346-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c5e916dc-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d7427ee0-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cdbea249-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4de82d70-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_84414409-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_96e405ab-3cc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_77e59a68-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_e336fb5e-3bc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_68c6e4ee-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cf80f537-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_85b16cee-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_b1a87a0f-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_efcff6c8-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7d5426eb-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d17ea55e-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5e3ced8e-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bfe916dc-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_907e649d-3fc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2d35ad65-48c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_e2e72871-3bc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1f869259-3ac7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_fbf61acf-3fc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_05525318-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4a1e6699-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7c31d57c-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_462349e0-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_dfc850c6-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_856ec10b-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_869eb6a4-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1d4cf5d2-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_0187216b-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_1f56e9f4-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c32122b2-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_f207981d-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_3a02b104-47c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d591d9e3-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_454edd04-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7f4f5454-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_899c8185-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c8e916dc-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7d019cbf-46c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6fc6e4ee-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c6bea249-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5b55ce46-3fc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_f9f52707-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_0e2cf81c-38c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7bc79e8b-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_5e989522-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_af67a63b-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_ff10b332-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_986f2886-41c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_cc7ea55e-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_3c346ba7-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_82cc1e26-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_6dc0d515-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_bc2122b2-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_15a8116e-3ec7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_c99b39b2-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_8cb14921-3bc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_a3cba4a3-3fc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d6360cea-43c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_2b890c97-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_3242a784-3dc7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_7f9c8185-44c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_d756dc64-45c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_4fca7c76-42c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_12d6954b-40c7-ed11-9174-005056a6f472.nc'\n",
      " '../data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_184caafe-46c7-ed11-9174-005056a6f472.nc']\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "for nc in \n",
    "    rootgrp = Dataset(\"test.nc\", \"w\", format=\"NETCDF4\")\n",
    "    print(rootgrp.data_model)\n",
    "    rootgrp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_file_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Create datasets and dataloaders\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CloudDataset(train_file_paths, train_labels, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[1;32m     60\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CloudDataset(val_file_paths, val_labels, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[1;32m     61\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CloudDataset(test_file_paths, test_labels, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_file_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Read the NetCDF file and extract image data\n",
    "        with nc.Dataset(file_path, 'r') as nc_file:\n",
    "            # Assuming your image data is stored in a variable named 'image_data'\n",
    "            image_data = nc_file.variables['image_data'][:]\n",
    "        \n",
    "        if self.transform:\n",
    "            # Perform any necessary transformations\n",
    "            image_data = self.transform(image_data)\n",
    "            \n",
    "        return image_data, label\n",
    "\n",
    "# Define hyperparameters\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the neural network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CloudDataset(train_file_paths, train_labels, transform=transforms.ToTensor())\n",
    "val_dataset = CloudDataset(val_file_paths, val_labels, transform=transforms.ToTensor())\n",
    "test_dataset = CloudDataset(test_file_paths, test_labels, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Validation Accuracy: {correct / total}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DATASET AND SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Convert to Torch\u001b[39;00m\n\u001b[1;32m     36\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m inputs_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(train_names)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m inputs_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(val_names)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m gts_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(train_gts)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_gts = np.load('skogs_gts_train.npy')\n",
    "val_gts = np.load('skogs_gts_val.npy')\n",
    "test_gts = np.load('skogs_gts_test.npy')\n",
    "\n",
    "train_json = np.load('skogs_json_train.npy', allow_pickle=True)\n",
    "val_json = np.load('skogs_json_val.npy', allow_pickle=True)\n",
    "test_json = np.load('skogs_json_test.npy', allow_pickle=True)\n",
    "\n",
    "train_names = np.load('skogs_names_train.npy')\n",
    "val_names = np.load('skogs_names_val.npy')\n",
    "test_names = np.load('skogs_names_test.npy')\n",
    "\n",
    "\n",
    "\n",
    "# GTS ÄR BINÄR DATA - TENSOR\n",
    "\n",
    "skogs_gts_test_loader = torch.utils.data.DataLoader(train_gts)\n",
    "skogs_gts_train_loader = torch.utils.data.DataLoader(test_gts)\n",
    "skogs_gts_val_loader = torch.utils.data.DataLoader(val_gts)\n",
    "\n",
    "# JSON ÄR METADATA - DOC {'Bilddatum': ['2019-09-19T00:00:00'], 'MedianvardeB2': tensor([0.0947]\n",
    "\n",
    "skogs_json_test_loader =torch.utils.data.DataLoader(test_json)\n",
    "skogs_json_train_loader =torch.utils.data.DataLoader(train_json)\n",
    "skogs_json_val_loader = torch.utils.data.DataLoader(val_json)\n",
    "\n",
    "# NAMES ÄR .NC FILER - LIST\n",
    "skogs_names_test_loader =torch.utils.data.DataLoader(train_names)\n",
    "skogs_names_train_loader = torch.utils.data.DataLoader(test_names)\n",
    "skogs_names_val_loader =torch.utils.data.DataLoader(val_names)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to Torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs_train = torch.Tensor(train_names).to(device)\n",
    "inputs_val = torch.Tensor(val_names).to(device)\n",
    "gts_train = torch.Tensor(train_gts).to(device)\n",
    "gts_val = torch.Tensor(val_gts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (ACC): 0.5000\n",
      "Precision (P): 0.5000\n",
      "Sensitivity (Sn): 1.0000\n",
      "Specificity (Sp): 0.0000\n",
      "F-score: 0.6667\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [0 5]]\n"
     ]
    }
   ],
   "source": [
    "# Första utkasst för att printa och fixa en matris, skriv gärna om\n",
    "# Example: Skriv om så den hämtar alla labeles\n",
    "y_true = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # Actual labels (1 for cloudy, 0 for not cloudy)\n",
    "y_pred = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Predicted labels\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate additional metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print additional metrics\n",
    "print(f\"Accuracy (ACC): {acc:.4f}\")\n",
    "print(f\"Precision (P): {precision:.4f}\")\n",
    "print(f\"Sensitivity (Sn): {recall:.4f}\")\n",
    "specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "print(f\"Specificity (Sp): {specificity:.4f}\")\n",
    "print(f\"F-score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix using seabornS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET (i believe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path for skogs_gts_test.npy: /path/to/your/npy/files/skogs_gts_test.npy\n",
      "Full path for skogs_gts_train.npy: /path/to/your/npy/files/skogs_gts_train.npy\n",
      "Full path for skogs_gts_val.npy: /path/to/your/npy/files/skogs_gts_val.npy\n",
      "Full path for skogs_json_test.npy: /path/to/your/npy/files/skogs_json_test.npy\n",
      "Full path for skogs_json_train.npy: /path/to/your/npy/files/skogs_json_train.npy\n",
      "Full path for skogs_json_val.npy: /path/to/your/npy/files/skogs_json_val.npy\n",
      "Full path for skogs_names_test.npy: /path/to/your/npy/files/skogs_names_test.npy\n",
      "Full path for skogs_names_train.npy: /path/to/your/npy/files/skogs_names_train.npy\n",
      "Full path for skogs_names_val.npy: /path/to/your/npy/files/skogs_names_val.npy\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "File: /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0b5101fb-44c7-ed11-9174-005056a6f472.nc\n",
      "Dimensions: ['time', 'y', 'x']\n",
      "Variables:\n",
      "  time:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 time(time)\n",
      "    units: seconds since 1970-01-01 00:00:00\n",
      "    standard_name: time\n",
      "    long_name: Time, unix time-stamp\n",
      "    axis: T\n",
      "    calendar: standard\n",
      "unlimited dimensions: \n",
      "current shape = (1,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  y:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 y(y)\n",
      "    units: metre\n",
      "    standard_name: projection_y_coordinate\n",
      "    long_name: y coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (21,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  x:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 x(x)\n",
      "    units: metre\n",
      "    standard_name: projection_x_coordinate\n",
      "    long_name: x coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (20,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  spatial_ref:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "int32 spatial_ref()\n",
      "    grid_mapping_name: transverse_mercator\n",
      "    scale_factor_at_central_meridian: 0.9996\n",
      "    longitude_of_central_meridian: 15.0\n",
      "    latitude_of_projection_origin: 0.0\n",
      "    false_easting: 500000.0\n",
      "    false_northing: 0.0\n",
      "    long_name: SWEREF99 TM\n",
      "    semi_major_axis: 6378137.0\n",
      "    semi_minor_axis: 6356752.314140356\n",
      "    inverse_flattening: 298.257222101\n",
      "    crs_wkt: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    spatial_ref: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    GeoTransform: [ 5.61770e+05 -1.00000e+01  0.00000e+00  6.55758e+06  0.00000e+00\n",
      "  1.00000e+01]\n",
      "unlimited dimensions: \n",
      "current shape = ()\n",
      "filling on, default _FillValue of -2147483647 used\n",
      "  b01:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b01(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b02:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b02(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b03:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b03(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b04:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b04(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b05:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b05(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b06:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b06(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b07:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b07(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b08:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b08(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b8a:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b8a(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b09:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b09(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  scl:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 scl(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b11:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b11(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b12:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b12(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "File: /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0ba812a4-39c7-ed11-9174-005056a6f472.nc\n",
      "Dimensions: ['time', 'y', 'x']\n",
      "Variables:\n",
      "  time:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 time(time)\n",
      "    units: seconds since 1970-01-01 00:00:00\n",
      "    standard_name: time\n",
      "    long_name: Time, unix time-stamp\n",
      "    axis: T\n",
      "    calendar: standard\n",
      "unlimited dimensions: \n",
      "current shape = (1,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  y:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 y(y)\n",
      "    units: metre\n",
      "    standard_name: projection_y_coordinate\n",
      "    long_name: y coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (21,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  x:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 x(x)\n",
      "    units: metre\n",
      "    standard_name: projection_x_coordinate\n",
      "    long_name: x coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (20,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  spatial_ref:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "int32 spatial_ref()\n",
      "    grid_mapping_name: transverse_mercator\n",
      "    scale_factor_at_central_meridian: 0.9996\n",
      "    longitude_of_central_meridian: 15.0\n",
      "    latitude_of_projection_origin: 0.0\n",
      "    false_easting: 500000.0\n",
      "    false_northing: 0.0\n",
      "    long_name: SWEREF99 TM\n",
      "    semi_major_axis: 6378137.0\n",
      "    semi_minor_axis: 6356752.314140356\n",
      "    inverse_flattening: 298.257222101\n",
      "    crs_wkt: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    spatial_ref: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    GeoTransform: [ 4.66090e+05 -1.00000e+01  0.00000e+00  6.96972e+06  0.00000e+00\n",
      "  1.00000e+01]\n",
      "unlimited dimensions: \n",
      "current shape = ()\n",
      "filling on, default _FillValue of -2147483647 used\n",
      "  b01:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b01(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b02:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b02(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b03:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b03(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b04:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b04(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b05:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b05(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b06:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b06(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b07:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b07(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b08:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b08(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b8a:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b8a(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b09:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b09(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  scl:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 scl(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b11:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b11(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b12:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b12(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "File: /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0d455c60-44c7-ed11-9174-005056a6f472.nc\n",
      "Dimensions: ['time', 'y', 'x']\n",
      "Variables:\n",
      "  time:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 time(time)\n",
      "    units: seconds since 1970-01-01 00:00:00\n",
      "    standard_name: time\n",
      "    long_name: Time, unix time-stamp\n",
      "    axis: T\n",
      "    calendar: standard\n",
      "unlimited dimensions: \n",
      "current shape = (1,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  y:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 y(y)\n",
      "    units: metre\n",
      "    standard_name: projection_y_coordinate\n",
      "    long_name: y coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (21,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  x:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 x(x)\n",
      "    units: metre\n",
      "    standard_name: projection_x_coordinate\n",
      "    long_name: x coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (20,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  spatial_ref:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "int32 spatial_ref()\n",
      "    grid_mapping_name: transverse_mercator\n",
      "    scale_factor_at_central_meridian: 0.9996\n",
      "    longitude_of_central_meridian: 15.0\n",
      "    latitude_of_projection_origin: 0.0\n",
      "    false_easting: 500000.0\n",
      "    false_northing: 0.0\n",
      "    long_name: SWEREF99 TM\n",
      "    semi_major_axis: 6378137.0\n",
      "    semi_minor_axis: 6356752.314140356\n",
      "    inverse_flattening: 298.257222101\n",
      "    crs_wkt: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    spatial_ref: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    GeoTransform: [ 5.61770e+05 -1.00000e+01  0.00000e+00  6.55758e+06  0.00000e+00\n",
      "  1.00000e+01]\n",
      "unlimited dimensions: \n",
      "current shape = ()\n",
      "filling on, default _FillValue of -2147483647 used\n",
      "  b01:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b01(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b02:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b02(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b03:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b03(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b04:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b04(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b05:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b05(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b06:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b06(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b07:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b07(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b08:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b08(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b8a:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b8a(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b09:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b09(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  scl:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 scl(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b11:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b11(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b12:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b12(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 21, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "File: /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0d067942-3cc7-ed11-9174-005056a6f472.nc\n",
      "Dimensions: ['time', 'y', 'x']\n",
      "Variables:\n",
      "  time:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 time(time)\n",
      "    units: seconds since 1970-01-01 00:00:00\n",
      "    standard_name: time\n",
      "    long_name: Time, unix time-stamp\n",
      "    axis: T\n",
      "    calendar: standard\n",
      "unlimited dimensions: \n",
      "current shape = (1,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  y:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 y(y)\n",
      "    units: metre\n",
      "    standard_name: projection_y_coordinate\n",
      "    long_name: y coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (20,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  x:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 x(x)\n",
      "    units: metre\n",
      "    standard_name: projection_x_coordinate\n",
      "    long_name: x coordinate of projection\n",
      "unlimited dimensions: \n",
      "current shape = (20,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  spatial_ref:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "int32 spatial_ref()\n",
      "    grid_mapping_name: transverse_mercator\n",
      "    scale_factor_at_central_meridian: 0.9996\n",
      "    longitude_of_central_meridian: 15.0\n",
      "    latitude_of_projection_origin: 0.0\n",
      "    false_easting: 500000.0\n",
      "    false_northing: 0.0\n",
      "    long_name: SWEREF99 TM\n",
      "    semi_major_axis: 6378137.0\n",
      "    semi_minor_axis: 6356752.314140356\n",
      "    inverse_flattening: 298.257222101\n",
      "    crs_wkt: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    spatial_ref: PROJCS[\"SWEREF99 TM\",GEOGCS[\"SWEREF99\",DATUM[\"SWEREF99\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6619\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4619\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3006\"]]\n",
      "    GeoTransform: [ 7.81440e+05 -1.00000e+01  0.00000e+00  7.50137e+06  0.00000e+00\n",
      "  1.00000e+01]\n",
      "unlimited dimensions: \n",
      "current shape = ()\n",
      "filling on, default _FillValue of -2147483647 used\n",
      "  b01:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b01(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b02:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b02(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b03:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b03(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b04:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b04(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b05:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b05(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b06:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b06(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b07:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b07(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b08:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b08(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b8a:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b8a(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b09:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b09(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  scl:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 scl(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b11:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b11(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "  b12:\n",
      "    <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 b12(time, y, x)\n",
      "    grid_mapping: spatial_ref\n",
      "    units: 1\n",
      "unlimited dimensions: \n",
      "current shape = (1, 20, 20)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming a base directory for the npy files, update this path as necessary\n",
    "BASE_PATH_NPY = '/path/to/your/npy/files'\n",
    "\n",
    "# Function to safely print the contents of a numpy array\n",
    "def print_array_contents(data, limit=5):\n",
    "    if data.size > limit:\n",
    "        print(data[:limit])\n",
    "        print(f\"... (and {data.size - limit} more items)\")\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "# Function to safely print the contents of a numpy array\n",
    "def print_array_contents(data, limit=5):\n",
    "    # If the data is too large, we only print a part of it\n",
    "    if data.size > limit:\n",
    "        print(data[:limit])\n",
    "        print(f\"... (and {data.size - limit} more items)\")\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "# Function to print the contents of a netCDF file\n",
    "def print_netcdf_contents(file_path):\n",
    "    full_path = os.path.abspath(file_path)  # Get the absolute path of the file\n",
    "    try:\n",
    "        dataset = nc.Dataset(full_path, 'r')\n",
    "        print(f\"File: {full_path}\")\n",
    "        print(f\"Dimensions: {list(dataset.dimensions.keys())}\")\n",
    "        print(\"Variables:\")\n",
    "        for var in dataset.variables.keys():\n",
    "            print(f\"  {var}:\")\n",
    "            print(f\"    {dataset.variables[var]}\")\n",
    "        dataset.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {full_path}: {e}\")\n",
    "\n",
    "# For npy files, assuming they're not netCDF files but you want to print their paths\n",
    "npy_file_names = [\n",
    "    'skogs_gts_test.npy',\n",
    "    'skogs_gts_train.npy',\n",
    "    'skogs_gts_val.npy',\n",
    "    'skogs_json_test.npy',\n",
    "    'skogs_json_train.npy',\n",
    "    'skogs_json_val.npy',\n",
    "    'skogs_names_test.npy',\n",
    "    'skogs_names_train.npy',\n",
    "    'skogs_names_val.npy'\n",
    "]\n",
    "\n",
    "# Print paths for npy files (assuming they're not meant to be read as netCDF files)\n",
    "for file_name in npy_file_names:\n",
    "    full_path = os.path.join(BASE_PATH_NPY, file_name)\n",
    "    print(f\"Full path for {file_name}: {full_path}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# List of relative or absolute netCDF file paths\n",
    "nc_file_paths = [\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0b5101fb-44c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0ba812a4-39c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0d455c60-44c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0d067942-3cc7-ed11-9174-005056a6f472.nc'\n",
    "]\n",
    "\n",
    "# Apply the function to each netCDF file\n",
    "for nc_path in nc_file_paths:\n",
    "    print_netcdf_contents(nc_path)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'skogs_names_test.npy/skogs_names_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Assuming 'skogs_names_test.npy' contains relative paths to the images\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m image_paths_test \u001b[38;5;241m=\u001b[39m load_image_paths(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Visualize 5 images from the test dataset\u001b[39;00m\n\u001b[1;32m     29\u001b[0m visualize_images(image_paths_test, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m, in \u001b[0;36mload_image_paths\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image_paths\u001b[39m(file_name):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, file_name), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'skogs_names_test.npy/skogs_names_test.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Adjust the base path to your dataset's location\n",
    "BASE_PATH_DATA = r'../data/'\n",
    "\n",
    "\n",
    "# Function to load image paths from a .npy file\n",
    "def load_image_paths(file_name):\n",
    "    return np.load(os.path.join(BASE_PATH_DATA, file_name), allow_pickle=True)\n",
    "\n",
    "# Function to visualize images\n",
    "def visualize_images(image_paths, title='Images', num_images=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.suptitle(title)\n",
    "    for i in range(min(num_images, len(image_paths))):\n",
    "        img_path = os.path.join(BASE_PATH_DATA, image_paths[i])\n",
    "        img = xr.open_dataset(img_path).to_array().mean(dim=0)  # Assuming multi-band images; using the mean for visualization\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'skogs_names_test.npy' contains relative paths to the images\n",
    "image_paths_test = load_image_paths('skogs_names_test.npy')\n",
    "\n",
    "# Visualize 5 images from the test dataset\n",
    "visualize_images(image_paths_test, title='Test Images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Implement a train model function so you can re_use it in task 3 and 4. \n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "    \n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = ...\n",
    "SHUFFLE = ...\n",
    "LEARNING_RATE = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move data and model to GPU (CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = ...\n",
    "SHUFFLE = ...\n",
    "LEARNING_RATE = ...\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
