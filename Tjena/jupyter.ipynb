{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKLIST\n",
    "* Fixa dataloaders\n",
    "    * Vafan är datan egentligen?\n",
    "    * \n",
    "* Fixa model\n",
    "    * Jag antar vi kör CNN (CSN plz)\n",
    "    * Prova andra arkitekturer?\n",
    "    *\n",
    "* Fixa hyperparametrar, kanske använda maskininlärning för det?\n",
    "* Fixa CUDA support och träna hemma?\n",
    "* Fixa det sista med matrisen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/skogsstyrelsen\\\\skogs_names_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m SPLIT_TO_USE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Read data + corresponding json info (incl ground truth)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m img_paths_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_PATH_DATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskogs_names_train.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m img_paths_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_val.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m      9\u001b[0m img_paths_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\malko\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/skogsstyrelsen\\\\skogs_names_train.npy'"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/skogsstyrelsen-data\"\n",
    "BASE_PATH_LOG = '../log'\n",
    "BASE_PATH_DATA = '../data/skogsstyrelsen'\n",
    "SPLIT_TO_USE = 'trainval' \n",
    "\n",
    "# Read data + corresponding json info (incl ground truth)\n",
    "img_paths_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_train.npy')))\n",
    "img_paths_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_val.npy')))\n",
    "img_paths_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy')))\n",
    "json_content_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_train.npy'), allow_pickle=True))\n",
    "json_content_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_val.npy'), allow_pickle=True))\n",
    "json_content_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_test.npy'), allow_pickle=True))\n",
    "# Run model on desired split\n",
    "\n",
    "if SPLIT_TO_USE == 'train':\n",
    "\timg_paths = img_paths_train\n",
    "\tjson_paths = json_content_train\n",
    "elif SPLIT_TO_USE == 'val':\n",
    "\timg_paths = img_paths_val\n",
    "\tjson_paths = json_content_val\n",
    "elif SPLIT_TO_USE == 'trainval':\n",
    "\timg_paths = img_paths_train + img_paths_val\n",
    "\tjson_paths = json_content_train + json_content_val\n",
    "elif SPLIT_TO_USE == 'test':\n",
    "\timg_paths = img_paths_test\n",
    "\tjson_paths = json_content_test\n",
    "all_binary_preds = []\n",
    "all_binary_gts = []\n",
    "for img_idx, img_path in enumerate(img_paths):\n",
    "\n",
    "\tprint(img_idx, len(img_paths))\n",
    "\n",
    "\t# Extract date to see if data is from before or after Jan 2022\n",
    "\t# (this affects the normalization used for the image)\n",
    "\timg = xr.open_dataset(img_path)\n",
    "\tyy_mm_dd = getattr(img, 'time').values[0]\n",
    "\tyy = yy_mm_dd.astype('datetime64[Y]').astype(int) + 1970\n",
    "\tmm = yy_mm_dd.astype('datetime64[M]').astype(int) % 12 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#Creates a dataset for our cloud images\n",
    "class CloudClassificationDataset(Dataset):\n",
    "    def init(self, imagedir, json, transform=None, channels:tuple=(\"b04\",\"b03\",\"b02\")):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.channels = channels\n",
    "        self.json = np.load(json,allow_pickle=True)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.json)\n",
    "\n",
    "    def getitem(self, index):\n",
    "        # Finds filename in json file and creates path to corresponding image\n",
    "        filename = \"skgs\"+self.json[index]['ValideringsobjektBildId']+\".nc\"\n",
    "        img_path = os.path.join(self.image_dir,filename)\n",
    "\n",
    "        # Opens NetCDF4 image\n",
    "        df = xr.open_dataset(img_path, engine='netcdf4')\n",
    "\n",
    "        # Extracts labels from the json file\n",
    "        label = self.json[index][\"MolnDis\"]\n",
    "\n",
    "        #Creates a tuple of dataArray objects with given channels from the tuple of channels and the dataArray\n",
    "        channel_lst=[]\n",
    "        for c in self.channels:\n",
    "            channel_lst.append(df[c][0])\n",
    "        channel_tup = tuple(channel_lst)\n",
    "\n",
    "        # Stacks, normalizes, and cut the image to a (20, 20, C) format between 0 and 1\n",
    "        imagergb = np.dstack(channel_tup)\n",
    "        cli=0\n",
    "        image = np.clip((imagergb-np.percentile(imagergb,cli))/(np.percentile(imagergb,100-cli)-np.percentile(imagergb,cli)),0, 1)[:20,:20,:]\n",
    "\n",
    "        return image,int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CloudClassificationDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "batch_size = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##Get the dataset of the CloudDataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData/skogsstyrelsen/skogs_json_train.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Print basic information about the data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\malko\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mc:\\Users\\malko\\.pyenv\\pyenv-win\\versions\\3.12.0\\Lib\\site-packages\\numpy\\lib\\format.py:795\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[1;32m--> 795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "##Get the dataset of the CloudDataset\n",
    "\n",
    "\n",
    "\n",
    "dataset = np.load(\"Data/skogsstyrelsen/skogs_json_train.npy\", allow_pickle=True)\n",
    "\n",
    "# Print basic information about the data\n",
    "print(\"Data Type:\", dataset.dtype)\n",
    "print(\"Shape of the Data:\", dataset.shape)\n",
    "print(\"First 5 Items in the Data:\", dataset[:5])\n",
    "\n",
    "# If the data contains unique labels or identifiers, you can print those as well\n",
    "if dataset.ndim == 1:  # Assuming it's a 1D array of labels\n",
    "    unique_labels = np.unique(dataset)\n",
    "    print(\"Unique Labels in the Data:\", unique_labels)\n",
    "#dataset = CloudClassificationDataset.CloudClassificationDataset(\"/Tjena/data/skogsstyrelsen/skogs_json_train.npy\",channels=(\"b04\",\"b03\",\"b02\"))\n",
    "\n",
    "##Creates validation/training dataset with 20/80 split\n",
    "validation_dataset, training_dataset = torch.utils.data.random_split(dataset,[0.2, 0.8])\n",
    "\n",
    "##Create a DataLoaders from the datasets.\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "##Creates an iterator and plots the pictures\n",
    "it = iter(training_loader)\n",
    "images, labels = next(it)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "\n",
    "print(labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "for nc in \n",
    "    rootgrp = Dataset(\"test.nc\", \"w\", format=\"NETCDF4\")\n",
    "    print(rootgrp.data_model)\n",
    "    rootgrp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Read the NetCDF file and extract image data\n",
    "        with nc.Dataset(file_path, 'r') as nc_file:\n",
    "            # Assuming your image data is stored in a variable named 'image_data'\n",
    "            image_data = nc_file.variables['image_data'][:]\n",
    "        \n",
    "        if self.transform:\n",
    "            # Perform any necessary transformations\n",
    "            image_data = self.transform(image_data)\n",
    "            \n",
    "        return image_data, label\n",
    "\n",
    "# Define hyperparameters\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the neural network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CloudDataset(train_file_paths, train_labels, transform=transforms.ToTensor())\n",
    "val_dataset = CloudDataset(val_file_paths, val_labels, transform=transforms.ToTensor())\n",
    "test_dataset = CloudDataset(test_file_paths, test_labels, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Validation Accuracy: {correct / total}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DATASET AND SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_gts = np.load('skogs_gts_train.npy')\n",
    "val_gts = np.load('skogs_gts_val.npy')\n",
    "test_gts = np.load('skogs_gts_test.npy')\n",
    "\n",
    "train_json = np.load('skogs_json_train.npy', allow_pickle=True)\n",
    "val_json = np.load('skogs_json_val.npy', allow_pickle=True)\n",
    "test_json = np.load('skogs_json_test.npy', allow_pickle=True)\n",
    "\n",
    "train_names = np.load('skogs_names_train.npy')\n",
    "val_names = np.load('skogs_names_val.npy')\n",
    "test_names = np.load('skogs_names_test.npy')\n",
    "\n",
    "\n",
    "\n",
    "# GTS ÄR BINÄR DATA - TENSOR\n",
    "\n",
    "skogs_gts_test_loader = torch.utils.data.DataLoader(train_gts)\n",
    "skogs_gts_train_loader = torch.utils.data.DataLoader(test_gts)\n",
    "skogs_gts_val_loader = torch.utils.data.DataLoader(val_gts)\n",
    "\n",
    "# JSON ÄR METADATA - DOC {'Bilddatum': ['2019-09-19T00:00:00'], 'MedianvardeB2': tensor([0.0947]\n",
    "\n",
    "skogs_json_test_loader =torch.utils.data.DataLoader(test_json)\n",
    "skogs_json_train_loader =torch.utils.data.DataLoader(train_json)\n",
    "skogs_json_val_loader = torch.utils.data.DataLoader(val_json)\n",
    "\n",
    "# NAMES ÄR .NC FILER - LIST\n",
    "skogs_names_test_loader =torch.utils.data.DataLoader(train_names)\n",
    "skogs_names_train_loader = torch.utils.data.DataLoader(test_names)\n",
    "skogs_names_val_loader =torch.utils.data.DataLoader(val_names)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to Torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs_train = torch.Tensor(train_names).to(device)\n",
    "inputs_val = torch.Tensor(val_names).to(device)\n",
    "gts_train = torch.Tensor(train_gts).to(device)\n",
    "gts_val = torch.Tensor(val_gts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Första utkasst för att printa och fixa en matris, skriv gärna om\n",
    "# Example: Skriv om så den hämtar alla labeles\n",
    "y_true = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # Actual labels (1 for cloudy, 0 for not cloudy)\n",
    "y_pred = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Predicted labels\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate additional metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print additional metrics\n",
    "print(f\"Accuracy (ACC): {acc:.4f}\")\n",
    "print(f\"Precision (P): {precision:.4f}\")\n",
    "print(f\"Sensitivity (Sn): {recall:.4f}\")\n",
    "specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "print(f\"Specificity (Sp): {specificity:.4f}\")\n",
    "print(f\"F-score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix using seabornS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET (i believe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming a base directory for the npy files, update this path as necessary\n",
    "BASE_PATH_NPY = '/path/to/your/npy/files'\n",
    "\n",
    "# Function to safely print the contents of a numpy array\n",
    "def print_array_contents(data, limit=5):\n",
    "    if data.size > limit:\n",
    "        print(data[:limit])\n",
    "        print(f\"... (and {data.size - limit} more items)\")\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "# Function to safely print the contents of a numpy array\n",
    "def print_array_contents(data, limit=5):\n",
    "    # If the data is too large, we only print a part of it\n",
    "    if data.size > limit:\n",
    "        print(data[:limit])\n",
    "        print(f\"... (and {data.size - limit} more items)\")\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "# Function to print the contents of a netCDF file\n",
    "def print_netcdf_contents(file_path):\n",
    "    full_path = os.path.abspath(file_path)  # Get the absolute path of the file\n",
    "    try:\n",
    "        dataset = nc.Dataset(full_path, 'r')\n",
    "        print(f\"File: {full_path}\")\n",
    "        print(f\"Dimensions: {list(dataset.dimensions.keys())}\")\n",
    "        print(\"Variables:\")\n",
    "        for var in dataset.variables.keys():\n",
    "            print(f\"  {var}:\")\n",
    "            print(f\"    {dataset.variables[var]}\")\n",
    "        dataset.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {full_path}: {e}\")\n",
    "\n",
    "# For npy files, assuming they're not netCDF files but you want to print their paths\n",
    "npy_file_names = [\n",
    "    'skogs_gts_test.npy',\n",
    "    'skogs_gts_train.npy',\n",
    "    'skogs_gts_val.npy',\n",
    "    'skogs_json_test.npy',\n",
    "    'skogs_json_train.npy',\n",
    "    'skogs_json_val.npy',\n",
    "    'skogs_names_test.npy',\n",
    "    'skogs_names_train.npy',\n",
    "    'skogs_names_val.npy'\n",
    "]\n",
    "\n",
    "# Print paths for npy files (assuming they're not meant to be read as netCDF files)\n",
    "for file_name in npy_file_names:\n",
    "    full_path = os.path.join(BASE_PATH_NPY, file_name)\n",
    "    print(f\"Full path for {file_name}: {full_path}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# List of relative or absolute netCDF file paths\n",
    "nc_file_paths = [\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0b5101fb-44c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0ba812a4-39c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0d455c60-44c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0d067942-3cc7-ed11-9174-005056a6f472.nc'\n",
    "]\n",
    "\n",
    "# Apply the function to each netCDF file\n",
    "for nc_path in nc_file_paths:\n",
    "    print_netcdf_contents(nc_path)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Adjust the base path to your dataset's location\n",
    "BASE_PATH_DATA = r'../data/'\n",
    "\n",
    "\n",
    "# Function to load image paths from a .npy file\n",
    "def load_image_paths(file_name):\n",
    "    return np.load(os.path.join(BASE_PATH_DATA, file_name), allow_pickle=True)\n",
    "\n",
    "# Function to visualize images\n",
    "def visualize_images(image_paths, title='Images', num_images=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.suptitle(title)\n",
    "    for i in range(min(num_images, len(image_paths))):\n",
    "        img_path = os.path.join(BASE_PATH_DATA, image_paths[i])\n",
    "        img = xr.open_dataset(img_path).to_array().mean(dim=0)  # Assuming multi-band images; using the mean for visualization\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'skogs_names_test.npy' contains relative paths to the images\n",
    "image_paths_test = load_image_paths('skogs_names_test.npy')\n",
    "\n",
    "# Visualize 5 images from the test dataset\n",
    "visualize_images(image_paths_test, title='Test Images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Implement a train model function so you can re_use it in task 3 and 4. \n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "    \n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = ...\n",
    "SHUFFLE = ...\n",
    "LEARNING_RATE = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move data and model to GPU (CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = ...\n",
    "SHUFFLE = ...\n",
    "LEARNING_RATE = ...\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\malko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/malko/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
