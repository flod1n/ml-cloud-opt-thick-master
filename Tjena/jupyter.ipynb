{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKLIST\n",
    "* Fixa dataloaders\n",
    "    * Vafan är datan egentligen?\n",
    "    * \n",
    "* Fixa model\n",
    "    * Jag antar vi kör CNN (CSN plz)\n",
    "    * Prova andra arkitekturer?\n",
    "    *\n",
    "* Fixa hyperparametrar, kanske använda maskininlärning för det?\n",
    "* Fixa CUDA support och träna hemma?\n",
    "* Fixa det sista med matrisen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import netCDF4\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungerande dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'MedianvardeB2': 0.094702356, 'MedianvardeB3': 0.11930823, 'MedianvardeB4': 0.15226941, 'MedianvardeB11': 0.29666, 'MedianvardeB12': 0.2657847, 'MedelvardeB2': 0.08789286, 'MedelvardeB3': 0.11191939, 'MedelvardeB4': 0.13810013, 'MedelvardeB11': 0.29279655, 'MedelvardeB12': 0.25594464, 'MinX': 561565.0, 'MinY': 6557585.0, 'MaxX': 561765.0, 'MaxY': 6557785.0, 'BkId': '65_5', 'MolnDis': '0'}, 0]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena\"\n",
    "BASE_PATH_LOG = '../log'\n",
    "BASE_PATH_DATA = '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data'\n",
    "SPLIT_TO_USE = 'trainval'\n",
    "\n",
    "SHUFFLE = True\n",
    "BATCH_SIZE = 15\n",
    "\n",
    "load_test = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_json_test.npy\",allow_pickle=True)\n",
    "load_train = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_json_train.npy\",allow_pickle=True) \n",
    "load_val = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_json_val.npy\",allow_pickle=True)\n",
    "\n",
    "load_labels_test = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_gts_test.npy\")\n",
    "load_labels_train = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_gts_train.npy\")\n",
    "load_labels_val = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_gts_val.npy\")\n",
    "\n",
    "load_name_test = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_name_test.npy\")\n",
    "load_name_train = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_name_train.npy\")\n",
    "load_name_val = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_name_val.npy\")\n",
    "\n",
    "\n",
    "# def img_extractor(dataset):\n",
    "#     dataset_lst = []\n",
    "#     for file in dataset:\n",
    "        \n",
    "\n",
    "#     return dataset_lst\n",
    "\n",
    "\n",
    "# Delete labels\n",
    "def delete_keys_in_dicts(list_of_dicts, keys_to_delete):\n",
    "    for dictionary in list_of_dicts:\n",
    "        for key in keys_to_delete:\n",
    "            if key in dictionary:\n",
    "                del dictionary[key]\n",
    "\n",
    "    return list_of_dicts\n",
    "\n",
    "keys_to_delete = [\"Bilddatum\",\"Processingdatum\",\"ValideringsobjektId\",\"BildUrl\",\"ValideringsobjektBildId\"]\n",
    "\n",
    "load_test = delete_keys_in_dicts(load_test,keys_to_delete)\n",
    "load_train = delete_keys_in_dicts(load_train,keys_to_delete)\n",
    "load_val = delete_keys_in_dicts(load_val, keys_to_delete)\n",
    "\n",
    "\n",
    "# Takes each dictionary and label into a list and returns it\n",
    "def merge_labels(dataset,labels):\n",
    "    i = 0\n",
    "    lst = []\n",
    "    dict = dataset[0]\n",
    "    while i < len(dataset):\n",
    "        pair = [dataset[i],labels[i]]\n",
    "        lst.append(pair)\n",
    "        i += 1\n",
    "    return lst\n",
    "\n",
    "load_test = merge_labels(load_test,load_labels_test)\n",
    "load_train = merge_labels(load_train,load_labels_train)\n",
    "load_val = merge_labels(load_val,load_labels_val)\n",
    "\n",
    "print(load_test[0])\n",
    "\n",
    "#Make dataloaders\n",
    "\n",
    "dataloader_test = DataLoader(load_test,batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloader_val = DataLoader(load_val,batch_size=BATCH_SIZE,shuffle=SHUFFLE)\n",
    "dataloader_train = DataLoader(load_train,batch_size=BATCH_SIZE,shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_765861ec-42c7-ed11-9174-005056a6f472.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_765861ec-42c7-ed11-9174-005056a6f472.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '1c5893bb-327a-4682-a075-40f902f818a3']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m load_name_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_names_train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m load_name_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_names_val.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m tjena \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(load_name_test[\u001b[38;5;241m0\u001b[39m],engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetcdf4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:566\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    555\u001b[0m     decode_cf,\n\u001b[1;32m    556\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 566\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    567\u001b[0m     filename_or_obj,\n\u001b[1;32m    568\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    572\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    573\u001b[0m     backend_ds,\n\u001b[1;32m    574\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:590\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    571\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    588\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    589\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 590\u001b[0m     store \u001b[38;5;241m=\u001b[39m NetCDF4DataStore\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    591\u001b[0m         filename_or_obj,\n\u001b[1;32m    592\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m    594\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m    595\u001b[0m         clobber\u001b[38;5;241m=\u001b[39mclobber,\n\u001b[1;32m    596\u001b[0m         diskless\u001b[38;5;241m=\u001b[39mdiskless,\n\u001b[1;32m    597\u001b[0m         persist\u001b[38;5;241m=\u001b[39mpersist,\n\u001b[1;32m    598\u001b[0m         lock\u001b[38;5;241m=\u001b[39mlock,\n\u001b[1;32m    599\u001b[0m         autoclose\u001b[38;5;241m=\u001b[39mautoclose,\n\u001b[1;32m    600\u001b[0m     )\n\u001b[1;32m    602\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    385\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    386\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    388\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    389\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(manager, group\u001b[38;5;241m=\u001b[39mgroup, mode\u001b[38;5;241m=\u001b[39mmode, lock\u001b[38;5;241m=\u001b[39mlock, autoclose\u001b[38;5;241m=\u001b[39mautoclose)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:338\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:400\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:394\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    395\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2469\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2028\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_765861ec-42c7-ed11-9174-005056a6f472.nc'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "load_name_test = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_names_test.npy\")\n",
    "load_name_train = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_names_train.npy\")\n",
    "load_name_val = np.load(\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data/skogs_names_val.npy\")\n",
    "\n",
    "tjena = xr.open_dataset(load_name_test[0],engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 332\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_db7d323a-3fc7-ed11-9174-005056a6f472.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_db7d323a-3fc7-ed11-9174-005056a6f472.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'd9b1f857-e738-4e72-bfe3-2d3e96fa2278']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_idx, \u001b[38;5;28mlen\u001b[39m(img_paths))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Extract date to see if data is from before or after Jan 2022\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# (this affects the normalization used for the image)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m img \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(img_path)\n\u001b[1;32m     36\u001b[0m yy_mm_dd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m yy \u001b[38;5;241m=\u001b[39m yy_mm_dd\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[Y]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1970\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:566\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    555\u001b[0m     decode_cf,\n\u001b[1;32m    556\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 566\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    567\u001b[0m     filename_or_obj,\n\u001b[1;32m    568\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    572\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    573\u001b[0m     backend_ds,\n\u001b[1;32m    574\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:590\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    571\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    588\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    589\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 590\u001b[0m     store \u001b[38;5;241m=\u001b[39m NetCDF4DataStore\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    591\u001b[0m         filename_or_obj,\n\u001b[1;32m    592\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m    594\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m    595\u001b[0m         clobber\u001b[38;5;241m=\u001b[39mclobber,\n\u001b[1;32m    596\u001b[0m         diskless\u001b[38;5;241m=\u001b[39mdiskless,\n\u001b[1;32m    597\u001b[0m         persist\u001b[38;5;241m=\u001b[39mpersist,\n\u001b[1;32m    598\u001b[0m         lock\u001b[38;5;241m=\u001b[39mlock,\n\u001b[1;32m    599\u001b[0m         autoclose\u001b[38;5;241m=\u001b[39mautoclose,\n\u001b[1;32m    600\u001b[0m     )\n\u001b[1;32m    602\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    385\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    386\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    388\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    389\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(manager, group\u001b[38;5;241m=\u001b[39mgroup, mode\u001b[38;5;241m=\u001b[39mmode, lock\u001b[38;5;241m=\u001b[39mlock, autoclose\u001b[38;5;241m=\u001b[39mautoclose)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:338\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:400\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:394\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    395\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2469\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2028\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/data/skogsstyrelsen/2A-netcdfs-cropped-from-nuria/skgs_db7d323a-3fc7-ed11-9174-005056a6f472.nc'"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena\"\n",
    "BASE_PATH_LOG = '../log'\n",
    "BASE_PATH_DATA = '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/data'\n",
    "SPLIT_TO_USE = 'trainval' \n",
    "\n",
    "# Read data + corresponding json info (incl ground truth)\n",
    "img_paths_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_train.npy')))\n",
    "img_paths_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_val.npy')))\n",
    "img_paths_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy')))\n",
    "json_content_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_train.npy'), allow_pickle=True))\n",
    "json_content_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_val.npy'), allow_pickle=True))\n",
    "json_content_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_test.npy'), allow_pickle=True))\n",
    "# Run model on desired split\n",
    "\n",
    "if SPLIT_TO_USE == 'train':\n",
    "\timg_paths = img_paths_train\n",
    "\tjson_paths = json_content_train\n",
    "elif SPLIT_TO_USE == 'val':\n",
    "\timg_paths = img_paths_val\n",
    "\tjson_paths = json_content_val\n",
    "elif SPLIT_TO_USE == 'trainval':\n",
    "\timg_paths = img_paths_train + img_paths_val\n",
    "\tjson_paths = json_content_train + json_content_val\n",
    "elif SPLIT_TO_USE == 'test':\n",
    "\timg_paths = img_paths_test\n",
    "\tjson_paths = json_content_test\n",
    "all_binary_preds = []\n",
    "all_binary_gts = []\n",
    "for img_idx, img_path in enumerate(img_paths):\n",
    "\n",
    "\tprint(img_idx, len(img_paths))\n",
    "\n",
    "\t# Extract date to see if data is from before or after Jan 2022\n",
    "\t# (this affects the normalization used for the image)\n",
    "\timg = xr.open_dataset(img_path)\n",
    "\tyy_mm_dd = getattr(img, 'time').values[0]\n",
    "\tyy = yy_mm_dd.astype('datetime64[Y]').astype(int) + 1970\n",
    "\tmm = yy_mm_dd.astype('datetime64[M]').astype(int) % 12 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "#Creates a dataset for our cloud images\n",
    "class CloudClassificationDataset(Dataset):\n",
    "    def init(self, imagedir, json, transform=None, channels:tuple=(\"b04\",\"b03\",\"b02\")):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.channels = channels\n",
    "        self.json = np.load(json,allow_pickle=True)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.json)\n",
    "\n",
    "    def getitem(self, index):\n",
    "        # Finds filename in json file and creates path to corresponding image\n",
    "        filename = \"skgs\"+self.json[index]['ValideringsobjektBildId']+\".nc\"\n",
    "        img_path = os.path.join(self.image_dir,filename)\n",
    "\n",
    "        # Opens NetCDF4 image\n",
    "        df = xr.open_dataset(img_path, engine='netcdf4')\n",
    "\n",
    "        # Extracts labels from the json file\n",
    "        label = self.json[index][\"MolnDis\"]\n",
    "\n",
    "        #Creates a tuple of dataArray objects with given channels from the tuple of channels and the dataArray\n",
    "        channel_lst=[]\n",
    "        for c in self.channels:\n",
    "            channel_lst.append(df[c][0])\n",
    "        channel_tup = tuple(channel_lst)\n",
    "\n",
    "        # Stacks, normalizes, and cut the image to a (20, 20, C) format between 0 and 1\n",
    "        imagergb = np.dstack(channel_tup)\n",
    "        cli=0\n",
    "        image = np.clip((imagergb-np.percentile(imagergb,cli))/(np.percentile(imagergb,100-cli)-np.percentile(imagergb,cli)),0, 1)[:20,:20,:]\n",
    "\n",
    "        return image,int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CloudClassificationDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'CloudClassificationDataset' has no attribute 'CloudClassificationDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##Get the dataset of the CloudDataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CloudClassificationDataset\u001b[38;5;241m.\u001b[39mCloudClassificationDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Tjena/data/skogsstyrelsen/skogs_json_train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m,channels\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb04\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb03\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb02\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m##Creates validation/training dataset with 20/80 split\u001b[39;00m\n\u001b[1;32m      5\u001b[0m validation_dataset, training_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(dataset,[\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'CloudClassificationDataset' has no attribute 'CloudClassificationDataset'"
     ]
    }
   ],
   "source": [
    "##Get the dataset of the CloudDataset\n",
    "dataset = CloudClassificationDataset.CloudClassificationDataset(\"/Tjena/data/skogsstyrelsen/skogs_json_train.npy\",channels=(\"b04\",\"b03\",\"b02\"))\n",
    "\n",
    "##Creates validation/training dataset with 20/80 split\n",
    "validation_dataset, training_dataset = torch.utils.data.random_split(dataset,[0.2, 0.8])\n",
    "\n",
    "##Create a DataLoaders from the datasets.\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "##Creates an iterator and plots the pictures\n",
    "it = iter(training_loader)\n",
    "images, labels = next(it)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "\n",
    "print(labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2607367069.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for nc in\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "for nc in \n",
    "    rootgrp = Dataset(\"test.nc\", \"w\", format=\"NETCDF4\")\n",
    "    print(rootgrp.data_model)\n",
    "    rootgrp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_file_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Create datasets and dataloaders\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CloudDataset(train_file_paths, train_labels, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[1;32m     60\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CloudDataset(val_file_paths, val_labels, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[1;32m     61\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CloudDataset(test_file_paths, test_labels, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_file_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Read the NetCDF file and extract image data\n",
    "        with nc.Dataset(file_path, 'r') as nc_file:\n",
    "            # Assuming your image data is stored in a variable named 'image_data'\n",
    "            image_data = nc_file.variables['image_data'][:]\n",
    "        \n",
    "        if self.transform:\n",
    "            # Perform any necessary transformations\n",
    "            image_data = self.transform(image_data)\n",
    "            \n",
    "        return image_data, label\n",
    "\n",
    "# Define hyperparameters\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the neural network architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CloudDataset(train_file_paths, train_labels, transform=transforms.ToTensor())\n",
    "val_dataset = CloudDataset(val_file_paths, val_labels, transform=transforms.ToTensor())\n",
    "test_dataset = CloudDataset(test_file_paths, test_labels, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Validation Accuracy: {correct / total}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DATASET AND SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'skogs_gts_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_gts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_gts_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m val_gts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_gts_val.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m test_gts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_gts_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'skogs_gts_train.npy'"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_gts = np.load('skogs_gts_train.npy')\n",
    "val_gts = np.load('skogs_gts_val.npy')\n",
    "test_gts = np.load('skogs_gts_test.npy')\n",
    "\n",
    "train_json = np.load('skogs_json_train.npy', allow_pickle=True)\n",
    "val_json = np.load('skogs_json_val.npy', allow_pickle=True)\n",
    "test_json = np.load('skogs_json_test.npy', allow_pickle=True)\n",
    "\n",
    "train_names = np.load('skogs_names_train.npy')\n",
    "val_names = np.load('skogs_names_val.npy')\n",
    "test_names = np.load('skogs_names_test.npy')\n",
    "\n",
    "\n",
    "\n",
    "# GTS ÄR BINÄR DATA - TENSOR\n",
    "\n",
    "skogs_gts_test_loader = torch.utils.data.DataLoader(train_gts)\n",
    "skogs_gts_train_loader = torch.utils.data.DataLoader(test_gts)\n",
    "skogs_gts_val_loader = torch.utils.data.DataLoader(val_gts)\n",
    "\n",
    "# JSON ÄR METADATA - DOC {'Bilddatum': ['2019-09-19T00:00:00'], 'MedianvardeB2': tensor([0.0947]\n",
    "\n",
    "skogs_json_test_loader =torch.utils.data.DataLoader(test_json)\n",
    "skogs_json_train_loader =torch.utils.data.DataLoader(train_json)\n",
    "skogs_json_val_loader = torch.utils.data.DataLoader(val_json)\n",
    "\n",
    "# NAMES ÄR .NC FILER - LIST\n",
    "skogs_names_test_loader =torch.utils.data.DataLoader(train_names)\n",
    "skogs_names_train_loader = torch.utils.data.DataLoader(test_names)\n",
    "skogs_names_val_loader =torch.utils.data.DataLoader(val_names)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to Torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs_train = torch.Tensor(train_names).to(device)\n",
    "inputs_val = torch.Tensor(val_names).to(device)\n",
    "gts_train = torch.Tensor(train_gts).to(device)\n",
    "gts_val = torch.Tensor(val_gts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (ACC): 0.5000\n",
      "Precision (P): 0.5000\n",
      "Sensitivity (Sn): 1.0000\n",
      "Specificity (Sp): 0.0000\n",
      "F-score: 0.6667\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [0 5]]\n"
     ]
    }
   ],
   "source": [
    "# Första utkasst för att printa och fixa en matris, skriv gärna om\n",
    "# Example: Skriv om så den hämtar alla labeles\n",
    "y_true = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # Actual labels (1 for cloudy, 0 for not cloudy)\n",
    "y_pred = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Predicted labels\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate additional metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print additional metrics\n",
    "print(f\"Accuracy (ACC): {acc:.4f}\")\n",
    "print(f\"Precision (P): {precision:.4f}\")\n",
    "print(f\"Sensitivity (Sn): {recall:.4f}\")\n",
    "specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "print(f\"Specificity (Sp): {specificity:.4f}\")\n",
    "print(f\"F-score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix using seabornS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET (i believe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path for skogs_gts_test.npy: /path/to/your/npy/files/skogs_gts_test.npy\n",
      "Full path for skogs_gts_train.npy: /path/to/your/npy/files/skogs_gts_train.npy\n",
      "Full path for skogs_gts_val.npy: /path/to/your/npy/files/skogs_gts_val.npy\n",
      "Full path for skogs_json_test.npy: /path/to/your/npy/files/skogs_json_test.npy\n",
      "Full path for skogs_json_train.npy: /path/to/your/npy/files/skogs_json_train.npy\n",
      "Full path for skogs_json_val.npy: /path/to/your/npy/files/skogs_json_val.npy\n",
      "Full path for skogs_names_test.npy: /path/to/your/npy/files/skogs_names_test.npy\n",
      "Full path for skogs_names_train.npy: /path/to/your/npy/files/skogs_names_train.npy\n",
      "Full path for skogs_names_val.npy: /path/to/your/npy/files/skogs_names_val.npy\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Could not read file /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0b5101fb-44c7-ed11-9174-005056a6f472.nc: [Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0b5101fb-44c7-ed11-9174-005056a6f472.nc'\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Could not read file /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0ba812a4-39c7-ed11-9174-005056a6f472.nc: [Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0ba812a4-39c7-ed11-9174-005056a6f472.nc'\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Could not read file /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0d455c60-44c7-ed11-9174-005056a6f472.nc: [Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0d455c60-44c7-ed11-9174-005056a6f472.nc'\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Could not read file /Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0d067942-3cc7-ed11-9174-005056a6f472.nc: [Errno 2] No such file or directory: '/Users/ludvigflodin/Code/ml-cloud-opt-thick-master/Tjena/2A-netcdfs-cropped-from-nuria/skgs_0d067942-3cc7-ed11-9174-005056a6f472.nc'\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming a base directory for the npy files, update this path as necessary\n",
    "BASE_PATH_NPY = '/path/to/your/npy/files'\n",
    "\n",
    "# Function to safely print the contents of a numpy array\n",
    "def print_array_contents(data, limit=5):\n",
    "    if data.size > limit:\n",
    "        print(data[:limit])\n",
    "        print(f\"... (and {data.size - limit} more items)\")\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "# Function to safely print the contents of a numpy array\n",
    "def print_array_contents(data, limit=5):\n",
    "    # If the data is too large, we only print a part of it\n",
    "    if data.size > limit:\n",
    "        print(data[:limit])\n",
    "        print(f\"... (and {data.size - limit} more items)\")\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "# Function to print the contents of a netCDF file\n",
    "def print_netcdf_contents(file_path):\n",
    "    full_path = os.path.abspath(file_path)  # Get the absolute path of the file\n",
    "    try:\n",
    "        dataset = nc.Dataset(full_path, 'r')\n",
    "        print(f\"File: {full_path}\")\n",
    "        print(f\"Dimensions: {list(dataset.dimensions.keys())}\")\n",
    "        print(\"Variables:\")\n",
    "        for var in dataset.variables.keys():\n",
    "            print(f\"  {var}:\")\n",
    "            print(f\"    {dataset.variables[var]}\")\n",
    "        dataset.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read file {full_path}: {e}\")\n",
    "\n",
    "# For npy files, assuming they're not netCDF files but you want to print their paths\n",
    "npy_file_names = [\n",
    "    'skogs_gts_test.npy',\n",
    "    'skogs_gts_train.npy',\n",
    "    'skogs_gts_val.npy',\n",
    "    'skogs_json_test.npy',\n",
    "    'skogs_json_train.npy',\n",
    "    'skogs_json_val.npy',\n",
    "    'skogs_names_test.npy',\n",
    "    'skogs_names_train.npy',\n",
    "    'skogs_names_val.npy'\n",
    "]\n",
    "\n",
    "# Print paths for npy files (assuming they're not meant to be read as netCDF files)\n",
    "for file_name in npy_file_names:\n",
    "    full_path = os.path.join(BASE_PATH_NPY, file_name)\n",
    "    print(f\"Full path for {file_name}: {full_path}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# List of relative or absolute netCDF file paths\n",
    "nc_file_paths = [\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0b5101fb-44c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0ba812a4-39c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0d455c60-44c7-ed11-9174-005056a6f472.nc',\n",
    "    '2A-netcdfs-cropped-from-nuria/skgs_0d067942-3cc7-ed11-9174-005056a6f472.nc'\n",
    "]\n",
    "\n",
    "# Apply the function to each netCDF file\n",
    "for nc_path in nc_file_paths:\n",
    "    print_netcdf_contents(nc_path)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'skogs_names_test.npy/skogs_names_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Assuming 'skogs_names_test.npy' contains relative paths to the images\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m image_paths_test \u001b[38;5;241m=\u001b[39m load_image_paths(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Visualize 5 images from the test dataset\u001b[39;00m\n\u001b[1;32m     29\u001b[0m visualize_images(image_paths_test, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m, in \u001b[0;36mload_image_paths\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image_paths\u001b[39m(file_name):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, file_name), allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'skogs_names_test.npy/skogs_names_test.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Adjust the base path to your dataset's location\n",
    "BASE_PATH_DATA = r'../data/'\n",
    "\n",
    "\n",
    "# Function to load image paths from a .npy file\n",
    "def load_image_paths(file_name):\n",
    "    return np.load(os.path.join(BASE_PATH_DATA, file_name), allow_pickle=True)\n",
    "\n",
    "# Function to visualize images\n",
    "def visualize_images(image_paths, title='Images', num_images=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.suptitle(title)\n",
    "    for i in range(min(num_images, len(image_paths))):\n",
    "        img_path = os.path.join(BASE_PATH_DATA, image_paths[i])\n",
    "        img = xr.open_dataset(img_path).to_array().mean(dim=0)  # Assuming multi-band images; using the mean for visualization\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'skogs_names_test.npy' contains relative paths to the images\n",
    "image_paths_test = load_image_paths('skogs_names_test.npy')\n",
    "\n",
    "# Visualize 5 images from the test dataset\n",
    "visualize_images(image_paths_test, title='Test Images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Implement a train model function so you can re_use it in task 3 and 4. \n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "    \n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = ...\n",
    "SHUFFLE = ...\n",
    "LEARNING_RATE = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move data and model to GPU (CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = ...\n",
    "SHUFFLE = ...\n",
    "LEARNING_RATE = ...\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
